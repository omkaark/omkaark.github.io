[{
    "name": "Why I made this website",
    "date": "2025-08-21"
}, {
    "name": "Why >> How",
    "image": "https://cdn.britannica.com/65/216665-050-A83A782E/Sisyphus-Titian-1548-49-Prado-Museum-Madrid.jpg",
    "date": "2025-08-22"
}, {
    "name": "Floating point maths made simple",
    "image": "3-float-math/floats.png",
    "slug": "fp",
    "date": "2025-09-02"
}, {
    "name": "The Illustrated LFM-2: Liquid AI",
    "image": "4-lfm/lfm2.png",
    "description": "An illustrated guide to LFM-2, a hybrid model from Liquid AI.",
    "date": "2025-09-03"
}, {
    "name": "So, why is attention quadratic?",
    "image": "5-quadratic-attention/qkt.png",
    "date": "2025-09-05"
}, {
    "name": "The Illustrated FastVLM: Apple",
    "image": "6-vit/vlm.png",
    "description": "An illustrated guide to FastVLM, a multimodal model by Apple that has some key innovations.",
    "slug": "fastvlm",
    "date": "2025-09-08"
}, {
    "name": "Decisive guide on Speculative Decoding",
    "image": "7-spec-decode/medusa.png",
    "description": "Speculative decoding is a common approach to making inference faster. This guide shows the 4 different approaches with illustrations and examples.",
    "slug": "speculative-decoding",
    "date": "2025-09-15"
}, {
    "name": "Cut Cross Entropy from first principles",
    "image": "8-cce/cce.png",
    "description": "Cross Entropy Loss is used in all language model training recipes. The old way of doing things required materializing the full logits, whereas Apple's Cut Cross Entropy eliminates that contributing to massive global memory savings.",
    "slug": "cce",
    "date": "2025-10-10"
}, {
    "name": "Cut Cross Entropy > Torch's version? No.",
    "image": "9-cce-impl/cce-run.png",
    "description": "I wanted to confirm the paper's findings and it turns out, for my experiment, AdamW has better loss over time than CCE on 4k steps.",
    "slug": "cce-experiment",
    "date": "2025-10-16"
}, {
    "name": "How to Train an LLM: Part 1",
    "image": "10-1b-model-p1/how-to-train-a-llm.png",
    "description": "I train a 1B llama3-like LLM from scratch and built the accompanying infra to train it.",
    "slug": "llm-1b-1",
    "date": "2025-11-11"
}, {
    "name": "How to Train an LLM: Part 2",
    "image": "11-1b-model-p2/how-to-train-a-llm-2.png",
    "description": "I optimize the infra and architecture to be much better.",
    "slug": "llm-1b-2",
    "date": "2025-11-13"
}, {
    "name": "How to Train an LLM: Part 3",
    "image": "11-1b-model-p2/how-to-train-a-llm-2.png",
    "description": "I run many experiments architectural and others.",
    "slug": "llm-1b-3",
    "date": "2025-11-13"
}]