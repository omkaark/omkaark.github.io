[Worklog] Training a 1B Llama3-like model

Ask Nishant Aklecha, Andrej Karpathy, Evan, Surya, Xavier, Faraz, Madhav Singhal, Any other cool people for review. Maybe someone from the toronto model club.

I want to create a poweful, open-source (weights, data, code) code completion model. I don’t have a good infra setup for it, so I decided to build this. Why go with the basic, ol’ llama3 with their config choices? because it has been reprod’d multiple times by multiple orgs and has good precedent.

I want to setup a simple test rig to run LLM experiments. This will be a simple worklog on running a training job with 8xH100s on a 1B llama3-like model. A lot of this is inspired by Karpathy’s Nanochat. Production systems don’t get made in one shot, they are developed over months as new needs are identified. For example, first base needs are setup, then custom kernels are developed, then fine-grain data mix support is added, blah blah. This repo is also on track to become production-ready, though it is very rudimentary right now. Over the months, I plan to focus on performance, supporting more dimensions of distributed training, etc.

One important characteristic of this blog is it’s a stream of thoughts getting validated in real time. I will put out ideas in thoughts and will validate that

My end goal is to have a capable dense 1B model that follows all the current best practices and is awesome for inference. The three blocks to get this right are Data, Architecture and Compute. My compute budget right now is a node of 8xH100, I think there is a good amount of kernel optimizations to be made for that and it’s a bit cheaper nowadays with everyone trying to rent blackwells out. I test all my infra on cheaper nodes to save money. I plan to stick to simple DDP but have a feeling I might need to go for FSDP.

For data, I will use available high-quality open source datasets for all parts of training (pre-, mid-, post-). For architecture, I plan to start off with the simple llama3 arch. While I know methods which work on small models don’t necessarily work on big models as well, I need a way to test out architectural and data quality changes for cheap. For that I will ablate and train a tiny model one H100 over a few hours. Llama3 will be my baseline. Furthermore, All of this will be deterministically set through seeds. For the smaller runs I will have full reproducibility which slows the run down a bit but that’s fine as it gives us an accurate ablation.```
 torch.use_deterministic_algorithms(True)
 torch.backends.cudnn.deterministic = True
 torch.backends.cudnn.benchmark = False
```

Simple transformer training math: 131072*2048+32*(2048+2048*2048*2+2048*512*2+2048+3*2048*8192)+2048 (put pic)

A simple way to get that in torch is `sum(param.numel() for param in model.parameters())` = 1241581568. 

Cool, total memory = (activations + weights + gradients + optimizer state) * 4 bytes per param + misc. 

Misc like kernels, rope cache, etc is 5GB.

weights = 1241581568 params
grads = 1241581568 params
activations = dependant on implementation (can be brought down with good kernels) lets say n * 1241581568. Empirically on a non-compiled model I get n around 13.5 for (1, 4096) input. it should scale linearly with batch size. 
optimizer = first moment + second moment = 2 * 1241581568.

Simply adding these up won’t give peak memory however, we care about the peak memory. This is the basic training process:```
loss = model.forward(tokens[:, :-1], targets=tokens[:, 1:]) # weights and activations occupy memory
loss.backward() # weights, grads and activations occupy memory
optim.step() # weights, grads and optimizer occupy memory.
```

Stepping through this, model.forward stores the weights + activations. backward() calculates gradients using activations and weights. optim.step() updates weights using optimizer states and grads. I tried a first pass with torch.compile (granted i ran it on my mac which doesn’t have cuda-level compile reductions), a very quick manual profile and it didn’t help with the memory, kernels it is but that comes later on H100s. [THIS IS WRONG, IN STEADY STATE grads, weights and optimizer states are stored all the time]

With the math shown above, we show that peak memory is used in loss.backward() which = weights + grads + activations. There is a known path to reducing activation memory: torch.compile and then, CUDA kernels for missed optimizations. Torch stores activations after most of it’s ops to calculate backwards, however, a lot of these ops can be fused, eliminating these intermediate activations! We will try torch compile again later, on an H100 this time.

Let’s talk about token budget (total tokens to train on). Scaling laws say a 1:20 ration of params:tokens is optimal. I have 1B params so 20B tokens is what I need to train on. My target batch size is 1M tokens because GPT-3 XL was trained on 1M (2^20) batch size, so I chose it pic ![](https://developer-blogs.nvidia.com/wp-content/uploads/2023/03/OpenAI-GPT-3.png). This means I will have 20000 steps to train my model through. Now, let’s do some maths with the naive non-compiled numbers above, I have 8xH100’s meaning 80gb per gpu. 1 batch of 4096 tokens takes 17.5 * 2214725632 * 4 in FP32 memory which is 144GB of memory.

