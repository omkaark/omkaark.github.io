<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
            <link rel="icon" type="image/x-icon" href="https://raw.githubusercontent.com/omkaark/omkaark.github.io/refs/heads/main/public/compass.jpg?raw=true">
            <title>The Illustrated FastVLM: Apple - Omkaar Kamath</title>
            <meta name="description" content="An illustrated guide to FastVLM, a multimodal model by Apple that has some key innovations.">
            <meta property="og:title" content="The Illustrated FastVLM: Apple - Omkaar Kamath">
            <meta property="og:description" content="An illustrated guide to FastVLM, a multimodal model by Apple that has some key innovations.">
            <meta property="og:type" content="article">
            <meta property="og:url" content="https://omkaark.com/posts/fastvlm.html">
            <meta property="og:image" content="https://raw.githubusercontent.com/omkaark/omkaark.github.io/refs/heads/main/public/6-vit/vlm.png?raw=true">
            <meta name="twitter:card" content="summary_large_image">
            <meta name="twitter:title" content="The Illustrated FastVLM: Apple - Omkaar Kamath">
            <meta name="twitter:description" content="An illustrated guide to FastVLM, a multimodal model by Apple that has some key innovations.">
            <meta name="twitter:image" content="https://raw.githubusercontent.com/omkaark/omkaark.github.io/refs/heads/main/public/6-vit/vlm.png?raw=true">
        

    <style>
    /* ---------- TYPOGRAPHY SYSTEM (rem/em only) ---------- */
:root {
  /* One knob to rule them all. Change this in media query to scale everything. */
  --base: 1rem;           /* 1rem = browser default (usually 16px) */
  --scale: 1.25;          /* modular scale ratio: Major Third */

  /* Precomputed steps off the base using the scale (no px anywhere) */
  --step-0:  calc(var(--base) * 1.25);
  --step-1:  calc(var(--base) * 1.4);
  --step-2:  calc(var(--step-1) * 1.4);
  --step-3:  calc(var(--step-2) * 1.4);
  --step-4:  calc(var(--step-3) * 1.7);
}

/* On mobile, set ONE number (the base) and everything scales automatically */
@media (max-width: 768px) {
  :root {
    --base: 0.95rem;   /* tighten all text ~5%. change to 0.90rem, 1.05rem, etc */
  }
}

/* ---------- GLOBAL ---------- */
html { font-size: 100%; } /* keep user/browser zoom respectful */
body {
  font-family: 'Courier New', 'Monaco', monospace;
  font-size: var(--step-0);
  line-height: 1.5;
  color: #000;
  background: #fff;
  width: 80vw;
  max-width: 50rem; /* 800px -> 50rem for consistency */
  margin: 0 auto;
  padding: 2.5rem 0; /* 40px -> 2.5rem */
}

/* ---------- HEADER ---------- */
.header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  margin-bottom: 2.5rem;
  border-bottom: 0.125rem solid #000; /* 2px -> rem */
  padding-bottom: 1.25rem;           /* 20px -> 1.25rem */
}

.name {
  font-size: var(--step-2);
  font-weight: 700;
  letter-spacing: 0.125rem; /* 2px -> rem */
  text-transform: uppercase;
}

/* ---------- INDEX / POSTS LIST ---------- */
.posts {
  margin-bottom: 5rem;
  font-size: var(--step-1);
}

.post {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 0.75rem 1.25rem; /* 12px 20px */
  margin: 1.25rem 0;       /* 20px */
  position: relative;
  overflow: hidden;
  cursor: pointer;
}

.post::before {
  content: '';
  position: absolute;
  inset: 0 auto 0 -100%;
  width: 100%;
  background: #FFFDD0;
  transition: left 0.15s ease;
  z-index: -1;
}
.post:hover::before { left: 0; }

.post-hover-gif {
  position: absolute;
  height: 4rem;
  width: 40rem;
  background-image: url("https://raw.githubusercontent.com/omkaark/omkaark.github.io/refs/heads/main/public/run.gif?raw=true");
  background-repeat: no-repeat;
  background-size: 100%;
  background-position: right;
  opacity: 0;
  pointer-events: none;
  z-index: 10000;
  transition: opacity 120ms ease-in;
}

.post-link:hover .post-hover-gif { 
  opacity: 1; 
}

.post-title { flex: 1; margin-right: 1.25rem; }
.post-link { color: #000; text-decoration: none; font-weight: 400; position: relative; }

.post-date {
  font-variant-numeric: tabular-nums;
  white-space: nowrap;
  font-weight: 700;
  font-size: var(--step-0);
}

/* ---------- FOOTER ---------- */
.footer {
  border-top: 0.125rem solid #000;
  padding-top: 1.25rem;
  display: flex;
  gap: 1.875rem; /* 30px */
}

.social-link {
  color: #000;
  text-decoration: none;
  text-transform: uppercase;
  letter-spacing: 0.0625rem; /* 1px */
  font-weight: 700;
  font-size: var(--step-0);
}
.social-link:hover { text-decoration: underline; }

/* ---------- POST PAGE ---------- */
.post-content {
  margin-bottom: 5rem;
  max-width: 50rem;
  font-size: var(--step-0);
  display: flex;
  flex-direction: column;
  align-items: center;
}

.post-meta {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 2.5rem;
  padding-bottom: 1.25rem;
  border-bottom: 0.0625rem solid #000; /* 1px */
}

.back-link {
  color: #000;
  text-decoration: none;
  text-transform: uppercase;
  letter-spacing: 0.0625rem;
  font-weight: 700;
  font-size: var(--step--1);
}
.back-link:hover { text-decoration: underline; }

/* Headings on the scale */
.post-content h1 {
  margin: 1.875rem 0 1.25rem; /* 30 0 20 */
  font-size: var(--step-3);
  line-height: 1.2;
}

.post-content h2 {
  margin: 1.875rem 0 0.9375rem; /* 30 0 15 */
  font-size: var(--step-2);
  line-height: 1.3;
  width: 100%;
}

.post-content h3 {
  margin: 1.5625rem 0 0.625rem; /* 25 0 10 */
  font-size: var(--step-1);
  line-height: 1.3;
  width: 100%;
}

.post-content h4 {
  margin: 1.425rem 0 0.425rem; /* 25 0 10 */
  font-size: var(--step-0);
  line-height: 1.25;
  width: 100%;
}

.post-content ul {
  width: 90%;
}

.post-content p {
  margin-bottom: 1.25rem; /* 20 */
  line-height: 1.6;
  font-size: var(--step-0);
  font-family: Monaco, monospace;
  width: 100%;
}

.post-content a { color: #000; text-decoration: underline; }
.post-content a:hover { background: #f5f5dc; }

/* Images unaffected by font scale, but spacings use rem for consistency */
.post-content img {
  margin: 1.25rem 0;
  max-width: 100%;
  max-height: 50rem;
  border-radius: 0.5rem; /* 8px */
  display: block;
  margin-left: auto;
  margin-right: auto;
  border: 1px #ccc dashed;
  padding: 10px;
}

/* ---------- INLINE CODE ---------- */
.post-content :not(pre) > code {
  font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
  font-size: var(--step-0);
  background: rgba(127,127,127,.12);
  border: 0.0625rem solid rgba(127,127,127,.25);
  padding: .15em .4em;   /* em ties to current font-size */
  border-radius: .35em;
  word-break: break-word;
}

/* ---------- CODE BLOCKS ---------- */
.post-content pre {
  margin: 1.25rem 0;
  padding: 0.875rem 1rem; /* 14px 16px */
  border-radius: 0.75rem; /* 12px */
  border: 0.0625rem solid rgba(127,127,127,.25);
  background: #f7f7f9;
  overflow: auto;
  -webkit-overflow-scrolling: touch;
  tab-size: 2;
  font-size: var(--step-0); /* control block base size */
  line-height: 1.55;
  width: 100%;
}
.post-content pre code {
  font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
  font-size: 1em;   /* inherit from pre; keep scale intact */
  line-height: 1.55;
  display: block;
  white-space: pre;
}

/* ---------- RESPONSIVE LAYOUT ONLY (typography scales via --base) ---------- */
@media (max-width: 768px) {
  body { width: 95vw; padding: 1.25rem 0; }

  .header { flex-direction: column; justify-items: center; }

  .post {
    flex-direction: column;
    align-items: flex-start;
    padding: 0.5rem 0.9375rem;
    margin: 1.25rem 0.625rem;
    background: #FFFDD0;
  }
  .post::before { display: none; }

  .post-title { margin-right: 0; margin-bottom: 0.25rem; }

  .footer { flex-wrap: wrap; gap: 0.9375rem; }

  .post-meta { flex-direction: column; align-items: flex-start; gap: 0.625rem; }

  .post-content { margin: 0 0.625rem; }

  .post-link:hover .post-hover-gif { 
    opacity: 0;
  }
}

    </style>
</head>
<body>
    <header class="header">
        <a href="https://omkaark.github.io" style="text-decoration: none;" class="social-link"><h1 class="name">Omkaar Kamath</h1></a>
        <div>
            <a href="https://twitter.com/omkizzy" class="social-link">Twitter</a>
            <a href="https://linkedin.com/in/omkaark" class="social-link">LinkedIn</a>
            <a href="https://github.com/omkaark" class="social-link">GitHub</a>
            <a href="https://youtube.com/@omkizzy" class="social-link">Youtube</a>
        </div>
    </header>

    
    <main class="post-content">
        <h1>The Illustrated FastVLM: Apple</h1>
<p>Apple's FastVLM is underhyped. It's a family of small models that pack a punch in terms of capability for their size. This project came to be because I wanted to dive deeper into VLMs. Walkthrough my <a href="https://github.com/omkaark/models-from-scratch/blob/main/apple-fastvlm/fastvlm.ipynb">code</a> (adapted from the <a href="https://arxiv.org/html/2412.13303v2#bib.bib53">paper</a>) along with this article for better understanding.</p>
<p>In today’s AI race, frontier models are often trained at massive costs and then open-sourced. By distilling larger models into smaller efficient architectures, you get cheap, high-quality models that run seamlessly on target hardware. This approach offloads the costly task of training huge teacher models, and makes it cheap to produce student models that are easier-to-align, faster, and hardware-friendly. By owning the hardware stack, Apple's incentive with this research is not only to ship better models, but also to guide future chip and software co-design.</p>
<p>This topic is susceptible to exploding in complexity, so I tightly limit scope to the architecture of the inference-time model and why this model is so efficient while foregoing details like training the model itself.</p>
<h2>FastVLM</h2>
<p>FYI a VLM = vision encoder -&gt; projector -&gt; LLM. ViT is the encoder backbone that takes an input image, runs preprocessing (will talk about this later), then forwards it through a deep network and finally, outputs one embedding per image token. These embeddings are to be concatenated with the language model's embedded input and ran through an LLM like in usual auto-regressive text gen.</p>
<p>Checkout the following top-level architecture from <a href="https://www.arxiv.org/pdf/2412.13303">Apple's FastVLM</a> paper showcasing the architecture.
<img src="https://raw.githubusercontent.com/omkaark/omkaark.github.io/refs/heads/main/public/6-vit/vlm.png?raw=true" alt="fastvlm" /></p>
<p>My <a href="https://github.com/omkaark/models-from-scratch/blob/main/apple-fastvlm/fastvlm.ipynb">code</a> has been segmented in the same sections as the image above which should make it easier to follow.</p>
<h2>Vision Encoder</h2>
<p>The vision encoder can be broken up into a few repeated blocks. Different colors in the ViT architecture below represent different blocks.</p>
<p><img style="max-height: 40rem !important;" src="https://raw.githubusercontent.com/omkaark/omkaark.github.io/refs/heads/main/public/6-vit/vit.png?raw=true" alt="Vision transformer architecture: preprocessing, stem, patch embeddings, positional embeddings, MobileOne head" /></p>
<h3>Preprocessing</h3>
<p>Preprocessing is simple, standardization across channel and cropping the image to (1024, 1024). The following code comes from my <a href="https://github.com/omkaark/models-from-scratch/blob/main/apple-fastvlm/fastvlm.ipynb">implementation</a>.</p>
<pre><code>image = Image.open('./image.jpg').convert(&quot;RGB&quot;)

img_tensor = torch.tensor(CLIPImageProcessor(
    crop_size={&quot;height&quot;: 1024, &quot;width&quot;: 1024},
    image_mean=[0.0, 0.0, 0.0],
    image_std=[1.0, 1.0, 1.0],
    size={&quot;shortest_edge&quot;: 1024},
    return_tensors='pt'
)(image)['pixel_values'])
</code></pre>
<p>Different ViTs handle high resolution differently. Some just patchify (split into fixed chunks), while others like FastViT-HD use a learned tokenizer (convs at the stem) to compress resolution while expanding channels.</p>
<h3>Stem</h3>
<p>The stem = 3×3 Conv + GeLU + 3×3 Depth-wise Conv + 1×1 Conv</p>
<p>Together, they shrink resolution and boost channels: [3, 1024, 1024] --&gt; [96, 256, 256].</p>
<p><img src="https://raw.githubusercontent.com/omkaark/omkaark.github.io/refs/heads/main/public/6-vit/stem.png?raw=true" alt="Stem block for vision model: 3x3 convolution, depthwise conv, 1x1 conv, GeLU activations, feature map outputs" /></p>
<p>Depth-wise convolutions + point-wise (1x1) convolutions are 5-10x more efficient than a standard convolution with the same kernel size. Think of the stem as a smart tokenizer.</p>
<h3>Stage 1-5</h3>
<p>The five stages after stem all look similar:</p>
<ul>
<li>Each has N blocks, each with:<ul>
<li>one TokenMixer (either conv-based or attention-based)</li>
<li>one ConvFFN (the MLP part)</li>
</ul>
</li>
<li>Each has a Patch Embed (downsampling)</li>
</ul>
<p>The 5 stages after stem are similar. Every stage has one Patch Embed and N blocks containing one TokenMixer (explained later) and one ConvFFN. In the first three stages, the TokenMixer is a 3×3 depth-wise conv (based on <a href="https://arxiv.org/pdf/2303.14189">RepMixer</a>). In stages 4–5, it switches to self-attention (after more downsampling). Structurally, this mirrors a transformer block: attention + MLP. Notice how TokenMixer + ConvFFN is structurally similar to the transformer decoder block (Attention + FFN).</p>
<p><img style="max-height: 40rem !important;" src="https://raw.githubusercontent.com/omkaark/omkaark.github.io/refs/heads/main/public/6-vit/stages.png?raw=true" alt="ConvFFN and token mixer block: 7x7 depthwise conv, batch norm, 1x1 convs, GeLU activation" /></p>
<p>The above is how one stage looks like. Stage 5 does not have a PatchEmbed layer.</p>
<p>Note: before stages 4 and 5, they introduce conditional positional embedding layer (7×7 depth-wise conv) to inject position information.</p>
<h4>&gt; Receptive field and Token mixers</h4>
<p>A receptive field is how much of the original image a feature can see. Therefore, convolutions inherently have local receptive fields (as big as the conv kernel) while self-attention (with no mask) is global. Because self-attention is an expensive operation, paper chooses to run the cheaper convolution token-mixer and downsample for the first three layers. Then, once the representation has been downsampled enough (8x in this case, [96, 256, 256] --&gt; [768, 32, 32]), it uses attention as a token mixture to increase receptive field while keeping compute overhead low.</p>
<h3>ViT head</h3>
<p>After the 5 stages, the encoder has a head that pools multi-scale features into final per-token embeddings the projector will consume.</p>
<p><img src="https://raw.githubusercontent.com/omkaark/omkaark.github.io/refs/heads/main/public/6-vit/head.png?raw=true" alt="Convolutional attention block with depthwise conv, pooling, reduce/expand conv, ReLU, Sigmoid, GeLU" /></p>
<h2>Projector</h2>
<p>Think of the projection (or “connector”) layer as a translator between the ViT’s embedding space and the LLM’s embedding space. This is a simple MLP, but surprisingly powerful: train just this connector well and you can get a vision encoder to play nicely with any LLM.</p>
<p><img src="https://raw.githubusercontent.com/omkaark/omkaark.github.io/refs/heads/main/public/6-vit/projector.png?raw=true" alt="Vision transformer projector: flatten and transpose, linear layers with GeLU activation, tensor shape transformations" /></p>
<h2>LLM</h2>
<p>This part is a standard LLM. Before tokenization, we look for <code>&lt;image&gt;</code> in the prompt, tokenize and embed everything before and after, then add the image embeddings in between, and finally pass it through the LLM as is normally done in auto-regressive text gen.</p>
<h2>Where does the efficiency come from?</h2>
<p>The goal of this paper was not to just be efficient, they wanted to allow efficient processing of high resolution (1024x1024) images.</p>
<p>Given FastViT-HD produces &gt;4x fewer tokens for a high-resolution image compared to other ViTs, LLM prefill time is relatively much lower.</p>
<p>Performing convolutions early for token-mixing and attention when representation is smaller cuts compute while upholding quality.</p>
<p>The convolution paths (stem, patch-embed, ConvFFN, RepMixer) use batchnorm which can be fused into a single convolution at inference. However, batchnorm might not generalize too well on out-of-distribution data.</p>
<p><img src="https://raw.githubusercontent.com/omkaark/omkaark.github.io/refs/heads/main/public/6-vit/efficient.png?raw=true" alt="Efficiency gains" /></p>
<p>As shown in the above graph from the FastVLM paper, for 1024x1024 resolution, FastVIT-HD has fast TTFT and performs well on evals.</p>
<h2>P.S.</h2>
<p>Some out of scope points I wanted to mention:</p>
<ul>
<li>The architecture of the train-time and inference-time model is different. They make use of over-parameterization and then fold identities and multiple convolutions into one convolution during inference time. It's in the FastViT <a href="https://arxiv.org/abs/2303.14189">paper</a>. I am writing an explanation for this next.</li>
<li>Training the model follows the same 2-stage training pipeline from <a href="https://arxiv.org/pdf/2310.03744">LLaVA-1.5</a>. During the first stage, only the projector is trained. In the second stage, they run SFT on the vision encoder, projector and the LLM.</li>
</ul>

    </main>
    
</body>
</html>