<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
            <link rel="icon" type="image/x-icon" href="https://raw.githubusercontent.com/omkaark/omkaark.github.io/refs/heads/main/public/compass.jpg?raw=true">
            <title>Decisive guide on Speculative Decoding - Omkaar Kamath</title>
            <meta name="description" content="Speculative decoding is a common approach to making inference faster. This guide shows the 4 different approaches with illustrations and examples.">
            <meta property="og:title" content="Decisive guide on Speculative Decoding - Omkaar Kamath">
            <meta property="og:description" content="Speculative decoding is a common approach to making inference faster. This guide shows the 4 different approaches with illustrations and examples.">
            <meta property="og:type" content="article">
            <meta property="og:url" content="https://omkaark.com/posts/speculative-decoding.html">
            <meta property="og:image" content="https://raw.githubusercontent.com/omkaark/omkaark.github.io/refs/heads/main/public/7-spec-decode/medusa.png?raw=true">
            <meta name="twitter:card" content="summary_large_image">
            <meta name="twitter:title" content="Decisive guide on Speculative Decoding - Omkaar Kamath">
            <meta name="twitter:description" content="Speculative decoding is a common approach to making inference faster. This guide shows the 4 different approaches with illustrations and examples.">
            <meta name="twitter:image" content="https://raw.githubusercontent.com/omkaark/omkaark.github.io/refs/heads/main/public/7-spec-decode/medusa.png?raw=true">
        

    <style>
    /* ---------- TYPOGRAPHY SYSTEM (rem/em only) ---------- */
:root {
  /* One knob to rule them all. Change this in media query to scale everything. */
  --base: 1rem;           /* 1rem = browser default (usually 16px) */
  --scale: 1.25;          /* modular scale ratio: Major Third */

  /* Precomputed steps off the base using the scale (no px anywhere) */
  --step-0:  calc(var(--base) * 1.25);
  --step-1:  calc(var(--base) * 1.4);
  --step-2:  calc(var(--step-1) * 1.4);
  --step-3:  calc(var(--step-2) * 1.4);
  --step-4:  calc(var(--step-3) * 1.7);
}

/* On mobile, set ONE number (the base) and everything scales automatically */
@media (max-width: 768px) {
  :root {
    --base: 0.95rem;   /* tighten all text ~5%. change to 0.90rem, 1.05rem, etc */
  }
}

/* ---------- GLOBAL ---------- */
html { font-size: 100%; } /* keep user/browser zoom respectful */
body {
  font-family: 'Courier New', 'Monaco', monospace;
  font-size: var(--step-0);
  line-height: 1.5;
  color: #000;
  background: #fff;
  width: 80vw;
  max-width: 50rem; /* 800px -> 50rem for consistency */
  margin: 0 auto;
  padding: 2.5rem 0; /* 40px -> 2.5rem */
}

/* ---------- HEADER ---------- */
.header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  margin-bottom: 2.5rem;
  border-bottom: 0.125rem solid #000; /* 2px -> rem */
  padding-bottom: 1.25rem;           /* 20px -> 1.25rem */
}

.name {
  font-size: var(--step-2);
  font-weight: 700;
  letter-spacing: 0.125rem; /* 2px -> rem */
  text-transform: uppercase;
}

/* ---------- INDEX / POSTS LIST ---------- */
.posts {
  margin-bottom: 5rem;
  font-size: var(--step-1);
}

.post {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 0.75rem 1.25rem; /* 12px 20px */
  margin: 1.25rem 0;       /* 20px */
  position: relative;
  overflow: hidden;
  cursor: pointer;
}

.post::before {
  content: '';
  position: absolute;
  inset: 0 auto 0 -100%;
  width: 100%;
  background: #FFFDD0;
  transition: left 0.15s ease;
  z-index: -1;
}
.post:hover::before { left: 0; }

.post-hover-gif {
  position: absolute;
  height: 4rem;
  width: 40rem;
  background-image: url("https://raw.githubusercontent.com/omkaark/omkaark.github.io/refs/heads/main/public/run.gif?raw=true");
  background-repeat: no-repeat;
  background-size: 100%;
  background-position: right;
  opacity: 0;
  pointer-events: none;
  z-index: 10000;
  transition: opacity 120ms ease-in;
}

.post-link:hover .post-hover-gif { 
  opacity: 1; 
}

.post-title { flex: 1; margin-right: 1.25rem; }
.post-link { color: #000; text-decoration: none; font-weight: 400; position: relative; }

.post-date {
  font-variant-numeric: tabular-nums;
  white-space: nowrap;
  font-weight: 700;
  font-size: var(--step-0);
}

/* ---------- FOOTER ---------- */
.footer {
  border-top: 0.125rem solid #000;
  padding-top: 1.25rem;
  display: flex;
  gap: 1.875rem; /* 30px */
}

.social-link {
  color: #000;
  text-decoration: none;
  text-transform: uppercase;
  letter-spacing: 0.0625rem; /* 1px */
  font-weight: 700;
  font-size: var(--step-0);
}
.social-link:hover { text-decoration: underline; }

/* ---------- POST PAGE ---------- */
.post-content {
  margin-bottom: 5rem;
  max-width: 50rem;
  font-size: var(--step-0);
  display: flex;
  flex-direction: column;
  align-items: center;
}

.post-meta {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 2.5rem;
  padding-bottom: 1.25rem;
  border-bottom: 0.0625rem solid #000; /* 1px */
}

.back-link {
  color: #000;
  text-decoration: none;
  text-transform: uppercase;
  letter-spacing: 0.0625rem;
  font-weight: 700;
  font-size: var(--step--1);
}
.back-link:hover { text-decoration: underline; }

/* Headings on the scale */
.post-content h1 {
  margin: 1.875rem 0 1.25rem; /* 30 0 20 */
  font-size: var(--step-3);
  line-height: 1.2;
}

.post-content h2 {
  margin: 1.875rem 0 0.9375rem; /* 30 0 15 */
  font-size: var(--step-2);
  line-height: 1.3;
  width: 100%;
}

.post-content h3 {
  margin: 1.5625rem 0 0.625rem; /* 25 0 10 */
  font-size: var(--step-1);
  line-height: 1.3;
  width: 100%;
}

.post-content h4 {
  margin: 1.425rem 0 0.425rem; /* 25 0 10 */
  font-size: var(--step-0);
  line-height: 1.25;
  width: 100%;
}

.post-content ul {
  width: 90%;
}

.post-content p {
  margin-bottom: 1.25rem; /* 20 */
  line-height: 1.6;
  font-size: var(--step-0);
  font-family: Monaco, monospace;
  width: 100%;
}

.post-content a { color: #000; text-decoration: underline; }
.post-content a:hover { background: #f5f5dc; }

/* Images unaffected by font scale, but spacings use rem for consistency */
.post-content img {
  margin: 1.25rem 0;
  max-width: 100%;
  max-height: 50rem;
  border-radius: 0.5rem; /* 8px */
  display: block;
  margin-left: auto;
  margin-right: auto;
  border: 1px #ccc dashed;
  padding: 10px;
}

/* ---------- INLINE CODE ---------- */
.post-content :not(pre) > code {
  font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
  font-size: var(--step-0);
  background: rgba(127,127,127,.12);
  border: 0.0625rem solid rgba(127,127,127,.25);
  padding: .15em .4em;   /* em ties to current font-size */
  border-radius: .35em;
  word-break: break-word;
}

/* ---------- CODE BLOCKS ---------- */
.post-content pre {
  margin: 1.25rem 0;
  padding: 0.875rem 1rem; /* 14px 16px */
  border-radius: 0.75rem; /* 12px */
  border: 0.0625rem solid rgba(127,127,127,.25);
  background: #f7f7f9;
  overflow: auto;
  -webkit-overflow-scrolling: touch;
  tab-size: 2;
  font-size: var(--step-0); /* control block base size */
  line-height: 1.55;
  width: 100%;
}
.post-content pre code {
  font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
  font-size: 1em;   /* inherit from pre; keep scale intact */
  line-height: 1.55;
  display: block;
  white-space: pre;
}

/* ---------- TABLES ---------- */
.post-content table {
  border-collapse: collapse;
  width: 100%;
  margin: 2rem 0;
  font-size: var(--step-0);
  font-family: Monaco, monospace;
}

.post-content th,
.post-content td {
  border: 0.0625rem solid #000; /* thin black grid */
  padding: 0.75rem 1rem;
  text-align: left;
  vertical-align: top;
}

.post-content th {
  background: #f5f5f5;
  font-weight: 700;
  letter-spacing: 0.03rem;
  text-transform: uppercase;
}

.post-content tr:nth-child(even) td {
  background: #fafafa;
}

.post-content tr:hover td {
  background: #FFFDD0; /* same yellow hover as posts */
  transition: background 120ms ease-in-out;
}

.post-content table code {
  font-size: 0.95em;
}

/* ---------- RESPONSIVE LAYOUT ONLY (typography scales via --base) ---------- */
@media (max-width: 768px) {
  body { width: 95vw; padding: 1.25rem 0; }

  .header { flex-direction: column; justify-items: center; }

  .post {
    flex-direction: column;
    align-items: flex-start;
    padding: 0.5rem 0.9375rem;
    margin: 1.25rem 0.625rem;
    background: #FFFDD0;
  }
  .post::before { display: none; }

  .post-title { margin-right: 0; margin-bottom: 0.25rem; }

  .footer { flex-wrap: wrap; gap: 0.9375rem; }

  .post-meta { flex-direction: column; align-items: flex-start; gap: 0.625rem; }

  .post-content { margin: 0 0.625rem; }

  .post-link:hover .post-hover-gif { 
    opacity: 0;
  }

  .post-content table {
    font-size: calc(var(--step-0) * 0.9);
  }

  .post-content th,
  .post-content td {
    padding: 0.6rem 0.75rem;
  }
}

    </style>
</head>
<body>
    <header class="header">
        <a href="https://omkaark.github.io" style="text-decoration: none;" class="social-link"><h1 class="name">Omkaar Kamath</h1></a>
        <div>
            <a href="https://twitter.com/omkizzy" class="social-link">Twitter</a>
            <a href="https://linkedin.com/in/omkaark" class="social-link">LinkedIn</a>
            <a href="https://github.com/omkaark" class="social-link">GitHub</a>
            <a href="https://youtube.com/@omkizzy" class="social-link">Youtube</a>
        </div>
    </header>

    
    <main class="post-content">
        <h1>Decisive guide on Speculative Decoding</h1>
<p>Recently, I've found myself looking into multi-batch inference frameworks like vLLM and techniques to infer on models faster. Some techniques require tweaking the architecture and others are systems.</p>
<p>One such technique is speculative decoding (SD). I wanted to explore this further and share an <a href="https://github.com/omkaark/speculative-decoding/blob/8c4b9c820bd0e6971d3bd3617f3d27c17e0d4400/qwen-draft-and-verify.py#L169">implementation</a>.</p>
<h2>What is speculative decoding (SD)?</h2>
<p>The idea is to somehow (more on this below) make educated guesses on the next <code>k</code> tokens and then, verify these guesses with a big model. Why does this work and what does &quot;verification&quot; mean in this context? I'll cover that later.</p>
<p>While I have not read all the literature in this field, I (with the help of deep research) have identified a few main themes which I explain in this blog.</p>
<p>This <a href="https://arxiv.org/pdf/2211.17192">paper</a> (Leviathan et al.) gave birth to SD and our first method works off it -&gt;</p>
<p><img src="https://storage.googleapis.com/gweb-research2023-media/media/SpeculativeDecoding-1-Illustration.mp4" alt="Google's Speculative Decoding Illustration" /></p>
<h3>Draft-And-Verify approach</h3>
<p>The draft model is a smaller version of the target model that we want to run inference on. Ex. Llama3-3b can be the draft model for Llama3-70b. The idea is simple, draft model generates <code>n</code> tokens. We feed the <code>n</code> tokens into target model which produces logits for previous tokens + one output 1 token. With simple checks against draft's vs target's logits produced, we accept or reject the produced tokens.</p>
<p>The caveat here is, during generation, speculative decoded output distribution is same as the target model's output distribution.</p>
<p><img src="https://raw.githubusercontent.com/omkaark/omkaark.github.io/refs/heads/main/public/7-spec-decode/draft-and-verify.png?raw=true" alt="Speculative decoding diagram: generate 4 draft tokens, verify with target model, accept or reject tokens" /></p>
<p>For demonstration, I implemented Qwen2-1.5B as target and Qwen2-0.5B as the draft.</p>
<p>I recommend reading my <a href="https://github.com/omkaark/speculative-decoding/blob/8c4b9c820bd0e6971d3bd3617f3d27c17e0d4400/qwen-draft-and-verify.py#L169">implementation code</a> (feel free to disregard my Qwen2 class). This is the pseudocode for the process:</p>
<pre><code class="language-python">while num_tokens_generated &lt; num_tokens_to_generate:
    k_to_draft = 4
    draft_tokens, draft_logits = draft_model(curr_tokens, n=k_to_draft)
    target_token, target_logits = target_model(curr_tokens + draft_tokens, n=1)

    let `accepted_tokens` be an empty list
    let `next_token` be an empty list

    for each draft_token:
        let `q_logits` be corresponding draft_logits to draft_token
        let `p_logits` be corresponding target_logits to draft_token

        let `qx` be corresponding softmax'd logit value to draft_token from q_logits
        let `px` be corresponding softmax'd logit value to draft_token from p_logits

        if qx &lt;= px then
            Add token to `accepted_tokens`
        else then
            if random_float() &lt; px/qx then
                Add token to `accepted_tokens`
            else then
                # reject any more samples
                residual = norm(max(p_logits - q_logits, 0))
                new_token = multinomial_sample(residual)
                break
    
    if len(accepted_tokens) = k_to_draft then
        new_token = target_token
    
    curr_tokens = new_token
</code></pre>
<p>I omitted cursor logic which makes sure we don't have to run prefill on the whole set of tokens everytime.</p>
<p>Simple KVCache optimization runs at 29 tok/s while draft-and-verify (with KVCache) runs at 20 tok/s (on my M4 Mac Book Air). This likely happens because the size difference is not monumental and the overhead involved in running two models make spec decode slower. Speedup depends on how many draft tokens are accepted on average and the compute cost ratio between draft and target models.</p>
<p>While I laid out a basic version, there are <a href="https://arxiv.org/abs/2211.17192?utm_source=chatgpt.com">many</a> <a href="https://arxiv.org/abs/2308.04623?utm_source=chatgpt.com">papers</a> <a href="https://arxiv.org/abs/2305.09781?utm_source=chatgpt.com">building</a> <a href="https://arxiv.org/abs/2310.08461?utm_source=chatgpt.com">on</a> <a href="https://arxiv.org/abs/2310.07177?utm_source=chatgpt.com">top</a> of this core idea. Other ideas include using a tiny distilled model, etc.</p>
<h3>N-gram approach</h3>
<p>N-gram is a simple approach which builds a mapping of n-gram -&gt; n-following-tokens. Whenever the last n generated tokens are in the mapping, you speculate the n-following tokens.</p>
<p>This image from vLLM's blog illustrates it perfectly.
<img src="https://blog.vllm.ai/assets/figures/spec-decode/figure3.png" alt="N-Gram speculative decoding" /></p>
<p>As you might point out, this only works well when there are specific repeated n-grams across prompt + generation which may not be the case in all types of use cases.</p>
<h3>Auxiliary Heads approach</h3>
<p>The <a href="https://arxiv.org/pdf/2401.10774">Medusa</a> paper describes adding <code>k</code> decoding heads which take the last hidden state's output as their inputs and output the hidden representation for the k+1 token. The diagram below should make it clear.</p>
<p><img src="https://raw.githubusercontent.com/omkaark/omkaark.github.io/refs/heads/main/public/7-spec-decode/medusa.png?verify=True" alt="Medusa decoding architecture with multiple linear heads predicting tokens in parallel from transformer outputs" /></p>
<h3>Feature Re-use approach</h3>
<p><a href="https://arxiv.org/html/2503.01840v1?utm_source=chatgpt.com">Eagle-3</a> is a relatively new paper (builds on top of EAGLE and EAGLE-2). I like their intuition... EAGLE-3 drops feature regression (predicting next token's features through regression) and does direct token prediction using a fused low/mid/high layer feature vector from the target. They report a 6.5× total speedup in their benchmarks. The following illustration is from the paper linked above.</p>
<p><img src="https://arxiv.org/html/2503.01840v1/x7.png" alt="Eagle-3" /></p>
<p>The draft model has to be pretrained jointly with frozen target model. Predicting a new draft token simply needs a re-run of the draft model with all the previous predicted draft tokens.</p>
<h2>Conclusion</h2>
<p>Speculative decoding pays off when there is a big draft–target gap (1B + 70B), the target model is expensive AND draft is close enough in quality that acceptance (verification) rate is high.</p>
<p>As an aside, this is Google AI search's results with and without speculative decoding:</p>
<p><img src="https://storage.googleapis.com/gweb-research2023-media/media/SpeculativeDecoding-0-AIO.mp4" alt="AI Search Results" /></p>
<p>This video and the video in the introduction are both from <a href="https://research.google/blog/looking-back-at-speculative-decoding/">Google's blog</a>.</p>
<h4>Sources</h4>
<ul>
<li><a href="https://blog.vllm.ai/2024/10/17/spec-decode.html?utm_source=chatgpt.com">vLLM's Blog on Speculative Decoding</a></li>
<li><a href="https://www.aleksagordic.com/blog/vllm">Aleksa's Blog on vLLM</a></li>
</ul>
<blockquote>
<p>By Omkaar Kamath</p>
</blockquote>

    </main>
    
</body>
</html>